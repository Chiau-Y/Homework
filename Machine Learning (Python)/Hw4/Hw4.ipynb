{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./hw4_train.csv') # donwload the train data\n",
    "X = data.iloc[:,0:data.shape[1]-1] # X_train\n",
    "y = data.Toughness # y_train\n",
    "\n",
    "X_test = pd.read_csv('./hw4_test.csv') # donwload the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0,test_size=0.2) # 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() # mean to zero, standard deviation to 1\n",
    "scaler.fit(np.array(y_train).reshape(-1, 1)) # set the model according to every feature respectively\n",
    "\n",
    "y_train_scaler = scaler.transform(np.array(y_train).reshape(-1, 1)) # StandardScaler\n",
    "y_val_scaler = scaler.transform(np.array(y_val).reshape(-1, 1)) # StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICLab_Brian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing scores :  0.916241925819211\n"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(max_depth=250, random_state=0)\n",
    "model.fit(X_train,y_train_scaler) # train\n",
    "\n",
    "# (1 - u/v), u=((y_true - y_pred) ** 2).sum(), v=((y_true - y_true.mean()) ** 2).sum()\n",
    "testing_score = model.score(X_val,y_val_scaler) \n",
    "print('testing scores : ',testing_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  0.09160529844185669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_val_pred = model.predict(X_val) # predict validation\n",
    "\n",
    "print('MSE : ',mean_squared_error(y_val_scaler,y_val_pred)) # calculate MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) # predict\n",
    "\n",
    "y_pred = scaler.inverse_transform(y_pred) # inverse StandardScaler\n",
    "\n",
    "y_pred_pd = pd.DataFrame(data=y_pred, columns=['Toughness'])\n",
    "y_pred_pd = y_pred_pd.reset_index()\n",
    "y_pred_pd.to_csv('n96084094_HW4_1.csv',index=False) # save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected (Dense)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential() # set model\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(64,))) # 16 output \n",
    "model.add(layers.Dense(16, activation='relu')) # 16 output \n",
    "model.add(layers.Dense(1, activation='linear')) # 1 output, y = a(wx + b), a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mean_squared_error', # regression problems\n",
    "              metrics=['mse']) # regression problems, MSE, MAE, MAPE, Cosine, not accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1630 - mse: 1.1630 - val_loss: 1.1278 - val_mse: 1.1278\n",
      "Epoch 2/250\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9940 - mse: 0.9940 - val_loss: 1.0313 - val_mse: 1.0313\n",
      "Epoch 3/250\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8798 - mse: 0.8798 - val_loss: 0.8857 - val_mse: 0.8857\n",
      "Epoch 4/250\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7319 - mse: 0.7319 - val_loss: 0.7252 - val_mse: 0.7252\n",
      "Epoch 5/250\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6057 - mse: 0.6057 - val_loss: 0.6046 - val_mse: 0.6046\n",
      "Epoch 6/250\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5155 - mse: 0.5155 - val_loss: 0.5209 - val_mse: 0.5209\n",
      "Epoch 7/250\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4480 - mse: 0.4480 - val_loss: 0.4509 - val_mse: 0.4509\n",
      "Epoch 8/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3961 - mse: 0.3961 - val_loss: 0.3990 - val_mse: 0.3990\n",
      "Epoch 9/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3503 - mse: 0.3503 - val_loss: 0.3543 - val_mse: 0.3543\n",
      "Epoch 10/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3113 - mse: 0.3113 - val_loss: 0.3101 - val_mse: 0.3101\n",
      "Epoch 11/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2769 - mse: 0.2769 - val_loss: 0.2802 - val_mse: 0.2802\n",
      "Epoch 12/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2498 - mse: 0.2498 - val_loss: 0.2469 - val_mse: 0.2469\n",
      "Epoch 13/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2288 - mse: 0.2288 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 14/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2099 - mse: 0.2099 - val_loss: 0.2048 - val_mse: 0.2048\n",
      "Epoch 15/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1940 - mse: 0.1940 - val_loss: 0.1905 - val_mse: 0.1905\n",
      "Epoch 16/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1813 - mse: 0.1813 - val_loss: 0.1739 - val_mse: 0.1739\n",
      "Epoch 17/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1696 - mse: 0.1696 - val_loss: 0.1637 - val_mse: 0.1637\n",
      "Epoch 18/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1603 - mse: 0.1603 - val_loss: 0.1555 - val_mse: 0.1555\n",
      "Epoch 19/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1516 - mse: 0.1516 - val_loss: 0.1483 - val_mse: 0.1483\n",
      "Epoch 20/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1441 - mse: 0.1441 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 21/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1387 - mse: 0.1387 - val_loss: 0.1344 - val_mse: 0.1344\n",
      "Epoch 22/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1354 - mse: 0.1354 - val_loss: 0.1272 - val_mse: 0.1272\n",
      "Epoch 23/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1299 - mse: 0.1299 - val_loss: 0.1311 - val_mse: 0.1311\n",
      "Epoch 24/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1254 - mse: 0.1254 - val_loss: 0.1234 - val_mse: 0.1234\n",
      "Epoch 25/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1221 - mse: 0.1221 - val_loss: 0.1173 - val_mse: 0.1173\n",
      "Epoch 26/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1203 - mse: 0.1203 - val_loss: 0.1161 - val_mse: 0.1161\n",
      "Epoch 27/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1154 - mse: 0.1154 - val_loss: 0.1167 - val_mse: 0.1167\n",
      "Epoch 28/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1152 - mse: 0.1152 - val_loss: 0.1189 - val_mse: 0.1189\n",
      "Epoch 29/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1121 - mse: 0.1121 - val_loss: 0.1072 - val_mse: 0.1072\n",
      "Epoch 30/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1101 - mse: 0.1101 - val_loss: 0.1039 - val_mse: 0.1039\n",
      "Epoch 31/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1046 - mse: 0.1046 - val_loss: 0.1087 - val_mse: 0.1087\n",
      "Epoch 32/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1046 - mse: 0.1046 - val_loss: 0.0998 - val_mse: 0.0998\n",
      "Epoch 33/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.1030 - val_mse: 0.1030\n",
      "Epoch 34/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.1062 - val_mse: 0.1062\n",
      "Epoch 35/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1011 - mse: 0.1011 - val_loss: 0.0967 - val_mse: 0.0967\n",
      "Epoch 36/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0955 - mse: 0.0955 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 37/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0944 - mse: 0.0944 - val_loss: 0.1015 - val_mse: 0.1015\n",
      "Epoch 38/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0923 - mse: 0.0923 - val_loss: 0.0942 - val_mse: 0.0942\n",
      "Epoch 39/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0932 - mse: 0.0932 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 40/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.0916 - val_mse: 0.0916\n",
      "Epoch 41/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.0880 - val_mse: 0.0880\n",
      "Epoch 42/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.0914 - val_mse: 0.0914\n",
      "Epoch 43/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 44/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0821 - mse: 0.0821 - val_loss: 0.0861 - val_mse: 0.0861\n",
      "Epoch 45/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0851 - mse: 0.0851 - val_loss: 0.0868 - val_mse: 0.0868\n",
      "Epoch 46/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0802 - mse: 0.0802 - val_loss: 0.0880 - val_mse: 0.0880\n",
      "Epoch 47/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0820 - mse: 0.0820 - val_loss: 0.0806 - val_mse: 0.0806\n",
      "Epoch 48/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0786 - mse: 0.0786 - val_loss: 0.0796 - val_mse: 0.0796\n",
      "Epoch 49/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0793 - mse: 0.0793 - val_loss: 0.0856 - val_mse: 0.0856\n",
      "Epoch 50/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0760 - mse: 0.0760 - val_loss: 0.0784 - val_mse: 0.0784\n",
      "Epoch 51/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0738 - mse: 0.0738 - val_loss: 0.0775 - val_mse: 0.0775\n",
      "Epoch 52/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0737 - mse: 0.0737 - val_loss: 0.0827 - val_mse: 0.0827\n",
      "Epoch 53/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0740 - mse: 0.0740 - val_loss: 0.0774 - val_mse: 0.0774\n",
      "Epoch 54/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.0745 - val_mse: 0.0745\n",
      "Epoch 55/250\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0722 - mse: 0.0722 - val_loss: 0.0738 - val_mse: 0.0738\n",
      "Epoch 56/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0694 - mse: 0.0694 - val_loss: 0.0728 - val_mse: 0.0728\n",
      "Epoch 57/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.0848 - val_mse: 0.0848\n",
      "Epoch 58/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.0801 - val_mse: 0.0801\n",
      "Epoch 59/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0666 - mse: 0.0666 - val_loss: 0.0768 - val_mse: 0.0768\n",
      "Epoch 60/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0686 - mse: 0.0686 - val_loss: 0.0701 - val_mse: 0.0701\n",
      "Epoch 61/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0646 - mse: 0.0646 - val_loss: 0.0695 - val_mse: 0.0695\n",
      "Epoch 62/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0654 - mse: 0.0654 - val_loss: 0.0719 - val_mse: 0.0719\n",
      "Epoch 63/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0630 - mse: 0.0630 - val_loss: 0.0734 - val_mse: 0.0734\n",
      "Epoch 64/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0652 - mse: 0.0652 - val_loss: 0.0685 - val_mse: 0.0685\n",
      "Epoch 65/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0628 - mse: 0.0628 - val_loss: 0.0668 - val_mse: 0.0668\n",
      "Epoch 66/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0623 - mse: 0.0623 - val_loss: 0.0679 - val_mse: 0.0679\n",
      "Epoch 67/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.0660 - val_mse: 0.0660\n",
      "Epoch 68/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0613 - mse: 0.0613 - val_loss: 0.0702 - val_mse: 0.0702\n",
      "Epoch 69/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0607 - mse: 0.0607 - val_loss: 0.0657 - val_mse: 0.0657\n",
      "Epoch 70/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0602 - mse: 0.0602 - val_loss: 0.0657 - val_mse: 0.0657\n",
      "Epoch 71/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0584 - mse: 0.0584 - val_loss: 0.0656 - val_mse: 0.0656\n",
      "Epoch 72/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0584 - mse: 0.0584 - val_loss: 0.0632 - val_mse: 0.0632\n",
      "Epoch 73/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0585 - mse: 0.0585 - val_loss: 0.0648 - val_mse: 0.0648\n",
      "Epoch 74/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0562 - mse: 0.0562 - val_loss: 0.0625 - val_mse: 0.0625\n",
      "Epoch 75/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0573 - mse: 0.0573 - val_loss: 0.0618 - val_mse: 0.0618\n",
      "Epoch 76/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0555 - mse: 0.0555 - val_loss: 0.0615 - val_mse: 0.0615\n",
      "Epoch 77/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0549 - mse: 0.0549 - val_loss: 0.0611 - val_mse: 0.0611\n",
      "Epoch 78/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0569 - mse: 0.0569 - val_loss: 0.0626 - val_mse: 0.0626\n",
      "Epoch 79/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0536 - mse: 0.0536 - val_loss: 0.0607 - val_mse: 0.0607\n",
      "Epoch 80/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0523 - mse: 0.0523 - val_loss: 0.0645 - val_mse: 0.0645\n",
      "Epoch 81/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0552 - mse: 0.0552 - val_loss: 0.0630 - val_mse: 0.0630\n",
      "Epoch 82/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0511 - mse: 0.0511 - val_loss: 0.0592 - val_mse: 0.0592\n",
      "Epoch 83/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0513 - mse: 0.0513 - val_loss: 0.0597 - val_mse: 0.0597\n",
      "Epoch 84/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0518 - mse: 0.0518 - val_loss: 0.0591 - val_mse: 0.0591\n",
      "Epoch 85/250\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0881 - val_mse: 0.0881\n",
      "Epoch 86/250\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0506 - mse: 0.0506 - val_loss: 0.0586 - val_mse: 0.0586\n",
      "Epoch 87/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.0619 - val_mse: 0.0619\n",
      "Epoch 88/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.0575 - val_mse: 0.0575\n",
      "Epoch 89/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0632 - val_mse: 0.0632\n",
      "Epoch 90/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0479 - mse: 0.0479 - val_loss: 0.0574 - val_mse: 0.0574\n",
      "Epoch 91/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.0584 - val_mse: 0.0584\n",
      "Epoch 92/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0476 - mse: 0.0476 - val_loss: 0.0568 - val_mse: 0.0568\n",
      "Epoch 93/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0481 - mse: 0.0481 - val_loss: 0.0567 - val_mse: 0.0567\n",
      "Epoch 94/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0473 - mse: 0.0473 - val_loss: 0.0569 - val_mse: 0.0569\n",
      "Epoch 95/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0456 - mse: 0.0456 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 96/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0472 - mse: 0.0472 - val_loss: 0.0548 - val_mse: 0.0548\n",
      "Epoch 97/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0563 - val_mse: 0.0563\n",
      "Epoch 98/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0464 - mse: 0.0464 - val_loss: 0.0547 - val_mse: 0.0547\n",
      "Epoch 99/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0471 - mse: 0.0471 - val_loss: 0.0572 - val_mse: 0.0572\n",
      "Epoch 100/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0465 - mse: 0.0465 - val_loss: 0.0568 - val_mse: 0.0568\n",
      "Epoch 101/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.0668 - val_mse: 0.0668\n",
      "Epoch 102/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0539 - val_mse: 0.0539\n",
      "Epoch 103/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0464 - mse: 0.0464 - val_loss: 0.0602 - val_mse: 0.0602\n",
      "Epoch 104/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0540 - val_mse: 0.0540\n",
      "Epoch 105/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.0547 - val_mse: 0.0547\n",
      "Epoch 106/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0537 - val_mse: 0.0537\n",
      "Epoch 107/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0440 - mse: 0.0440 - val_loss: 0.0550 - val_mse: 0.0550\n",
      "Epoch 108/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0545 - val_mse: 0.0545\n",
      "Epoch 109/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0564 - val_mse: 0.0564\n",
      "Epoch 110/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.0537 - val_mse: 0.0537\n",
      "Epoch 111/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0531 - val_mse: 0.0531\n",
      "Epoch 112/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0554 - val_mse: 0.0554\n",
      "Epoch 113/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0555 - val_mse: 0.0555\n",
      "Epoch 114/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0649 - val_mse: 0.0649\n",
      "Epoch 115/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0557 - val_mse: 0.0557\n",
      "Epoch 116/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0579 - val_mse: 0.0579\n",
      "Epoch 117/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0601 - val_mse: 0.0601\n",
      "Epoch 118/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0536 - val_mse: 0.0536\n",
      "Epoch 119/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0560 - val_mse: 0.0560\n",
      "Epoch 120/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0581 - val_mse: 0.0581\n",
      "Epoch 121/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0518 - val_mse: 0.0518\n",
      "Epoch 122/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0777 - val_mse: 0.0777\n",
      "Epoch 123/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0561 - val_mse: 0.0561\n",
      "Epoch 124/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0543 - val_mse: 0.0543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0613 - val_mse: 0.0613\n",
      "Epoch 126/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0515 - val_mse: 0.0515\n",
      "Epoch 127/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0539 - val_mse: 0.0539\n",
      "Epoch 128/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0525 - val_mse: 0.0525\n",
      "Epoch 129/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0526 - val_mse: 0.0526\n",
      "Epoch 130/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0532 - val_mse: 0.0532\n",
      "Epoch 131/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0522 - val_mse: 0.0522\n",
      "Epoch 132/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0523 - val_mse: 0.0523\n",
      "Epoch 133/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0540 - val_mse: 0.0540\n",
      "Epoch 134/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0629 - val_mse: 0.0629\n",
      "Epoch 135/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0556 - val_mse: 0.0556\n",
      "Epoch 136/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0536 - val_mse: 0.0536\n",
      "Epoch 137/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0518 - val_mse: 0.0518\n",
      "Epoch 138/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0651 - val_mse: 0.0651\n",
      "Epoch 139/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0556 - val_mse: 0.0556\n",
      "Epoch 140/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0523 - val_mse: 0.0523\n",
      "Epoch 141/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0671 - val_mse: 0.0671\n",
      "Epoch 142/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0521 - val_mse: 0.0521\n",
      "Epoch 143/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0522 - val_mse: 0.0522\n",
      "Epoch 144/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0510 - val_mse: 0.0510\n",
      "Epoch 145/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0602 - val_mse: 0.0602\n",
      "Epoch 146/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0562 - val_mse: 0.0562\n",
      "Epoch 147/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0533 - val_mse: 0.0533\n",
      "Epoch 148/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0531 - val_mse: 0.0531\n",
      "Epoch 149/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0512 - val_mse: 0.0512\n",
      "Epoch 150/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0541 - val_mse: 0.0541\n",
      "Epoch 151/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0733 - val_mse: 0.0733\n",
      "Epoch 152/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0527 - val_mse: 0.0527\n",
      "Epoch 153/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0524 - val_mse: 0.0524\n",
      "Epoch 154/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0507 - val_mse: 0.0507\n",
      "Epoch 155/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0529 - val_mse: 0.0529\n",
      "Epoch 156/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0544 - val_mse: 0.0544\n",
      "Epoch 157/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0559 - val_mse: 0.0559\n",
      "Epoch 158/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 159/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0516 - val_mse: 0.0516\n",
      "Epoch 160/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 161/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0656 - val_mse: 0.0656\n",
      "Epoch 162/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0594 - val_mse: 0.0594\n",
      "Epoch 163/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0552 - val_mse: 0.0552\n",
      "Epoch 164/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0540 - val_mse: 0.0540\n",
      "Epoch 165/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0513 - val_mse: 0.0513\n",
      "Epoch 166/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0531 - val_mse: 0.0531\n",
      "Epoch 167/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0656 - val_mse: 0.0656\n",
      "Epoch 168/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0610 - val_mse: 0.0610\n",
      "Epoch 169/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0515 - val_mse: 0.0515\n",
      "Epoch 170/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0514 - val_mse: 0.0514\n",
      "Epoch 171/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0688 - val_mse: 0.0688\n",
      "Epoch 172/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0499 - val_mse: 0.0499\n",
      "Epoch 173/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0531 - val_mse: 0.0531\n",
      "Epoch 174/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0504 - val_mse: 0.0504\n",
      "Epoch 175/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0696 - val_mse: 0.0696\n",
      "Epoch 176/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0523 - val_mse: 0.0523\n",
      "Epoch 177/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0514 - val_mse: 0.0514\n",
      "Epoch 178/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0500 - val_mse: 0.0500\n",
      "Epoch 179/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0508 - val_mse: 0.0508\n",
      "Epoch 180/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0504 - val_mse: 0.0504\n",
      "Epoch 181/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0513 - val_mse: 0.0513\n",
      "Epoch 182/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0582 - val_mse: 0.0582\n",
      "Epoch 183/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0525 - val_mse: 0.0525\n",
      "Epoch 184/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0500 - val_mse: 0.0500\n",
      "Epoch 185/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 186/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0485 - val_mse: 0.0485\n",
      "Epoch 187/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 188/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0501 - val_mse: 0.0501\n",
      "Epoch 189/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 190/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0494 - val_mse: 0.0494\n",
      "Epoch 191/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0530 - val_mse: 0.0530\n",
      "Epoch 192/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0519 - val_mse: 0.0519\n",
      "Epoch 193/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0533 - val_mse: 0.0533\n",
      "Epoch 194/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0496 - val_mse: 0.0496\n",
      "Epoch 195/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0496 - val_mse: 0.0496\n",
      "Epoch 196/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0497 - val_mse: 0.0497\n",
      "Epoch 197/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0494 - val_mse: 0.0494\n",
      "Epoch 198/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0504 - val_mse: 0.0504\n",
      "Epoch 199/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0500 - val_mse: 0.0500\n",
      "Epoch 200/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0487 - val_mse: 0.0487\n",
      "Epoch 201/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 202/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0491 - val_mse: 0.0491\n",
      "Epoch 203/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0619 - val_mse: 0.0619\n",
      "Epoch 204/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0487 - val_mse: 0.0487\n",
      "Epoch 205/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0506 - val_mse: 0.0506\n",
      "Epoch 206/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0508 - val_mse: 0.0508\n",
      "Epoch 207/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0625 - val_mse: 0.0625\n",
      "Epoch 208/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0517 - val_mse: 0.0517\n",
      "Epoch 209/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 210/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.0520 - val_mse: 0.0520\n",
      "Epoch 211/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0490 - val_mse: 0.0490\n",
      "Epoch 212/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0483 - val_mse: 0.0483\n",
      "Epoch 213/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.0510 - val_mse: 0.0510\n",
      "Epoch 214/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0489 - val_mse: 0.0489\n",
      "Epoch 215/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0485 - val_mse: 0.0485\n",
      "Epoch 216/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0530 - val_mse: 0.0530\n",
      "Epoch 217/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.0485 - val_mse: 0.0485\n",
      "Epoch 218/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0496 - val_mse: 0.0496\n",
      "Epoch 219/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0550 - val_mse: 0.0550\n",
      "Epoch 220/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 221/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 222/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0571 - val_mse: 0.0571\n",
      "Epoch 223/250\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.0500 - val_mse: 0.0500\n",
      "Epoch 224/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0595 - val_mse: 0.0595\n",
      "Epoch 225/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.0479 - val_mse: 0.0479\n",
      "Epoch 226/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.0506 - val_mse: 0.0506\n",
      "Epoch 227/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0552 - val_mse: 0.0552\n",
      "Epoch 228/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0571 - val_mse: 0.0571\n",
      "Epoch 229/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0559 - val_mse: 0.0559\n",
      "Epoch 230/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0491 - val_mse: 0.0491\n",
      "Epoch 231/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 232/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0517 - val_mse: 0.0517\n",
      "Epoch 233/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 234/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.0483 - val_mse: 0.0483\n",
      "Epoch 235/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0505 - val_mse: 0.0505\n",
      "Epoch 236/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0311 - mse: 0.0311 - val_loss: 0.0558 - val_mse: 0.0558\n",
      "Epoch 237/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0485 - val_mse: 0.0485\n",
      "Epoch 238/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.0484 - val_mse: 0.0484\n",
      "Epoch 239/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.0560 - val_mse: 0.0560\n",
      "Epoch 240/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0539 - val_mse: 0.0539\n",
      "Epoch 241/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0318 - mse: 0.0318 - val_loss: 0.0499 - val_mse: 0.0499\n",
      "Epoch 242/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.0526 - val_mse: 0.0526\n",
      "Epoch 243/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.0492 - val_mse: 0.0492\n",
      "Epoch 244/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0309 - mse: 0.0309 - val_loss: 0.0479 - val_mse: 0.0479\n",
      "Epoch 245/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0496 - val_mse: 0.0496\n",
      "Epoch 246/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0491 - val_mse: 0.0491\n",
      "Epoch 247/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.0542 - val_mse: 0.0542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.0491 - val_mse: 0.0491\n",
      "Epoch 249/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.0492 - val_mse: 0.0492\n",
      "Epoch 250/250\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.0487 - val_mse: 0.0487\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_scaler, epochs=250, batch_size=1000, validation_data=(X_val, y_val_scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1) # x-axis\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')  # y-axis, loss\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss') # y-axis, , loss\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred=scaler.inverse_transform(y_pred)  # inverse StandardScaler\n",
    "\n",
    "y_pred_pd = pd.DataFrame(data=y_pred, columns=['Toughness'])\n",
    "y_pred_pd = y_pred_pd.reset_index()\n",
    "y_pred_pd.to_csv('n96084094_HW4_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
